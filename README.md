# FRANC: Fusion of bRANching Concepts ( NTU DLCV Fall 2024 Final project 2: Multiple Concept Personalization )

## Clone this repo
```shell
$ git clone https://github.com/DLCV-Fall-2024/DLCV-Fall-2024-Final-2-dontlookcanvis.git
```

## Preparation

**Install environment**

In the repository folder,
```shell
$ pip install -r requirements.txt --extra-index-url https://download.pytorch.org/whl/cu118
```

**Download Pretrained Model**
```shell
$ cd experiments/pretrained_models
```
Execute in `./experiments/pretrained_models`
```shell
$ git-lfs clone https://huggingface.co/windwhinny/chilloutmix.git
$ git clone https://huggingface.co/sd-legacy/stable-diffusion-v1-5
$ git-lfs clone https://huggingface.co/SG161222/RealVisXL_V5.0.git
```
After downloading the RealVisXL V5.0 model, please convert the safetensors to binary files as:
```shell
$ python3 safetensor2bin.py
```

**Download Checkpoints**
You can download all the checkpoints except for SDXL from: https://drive.google.com/drive/folders/1u2un5sqWEY7yj-U-1wgyN41BICqQ3wWm?usp=sharing.

You can also use ED-LoRA directly from this repository; only merged LoRAs need downloads.

For downloading SDXL checkpoints, use
```shell
$ bash download.sh
```

## Training
Directly use the provided training config files, or generated needed files and configuration from template as:
``` shell
$ python3 scripts/data_format.py --input_images <path/to/images/folder> \
                                --input_json_path <path/to/annotation/file> \
                                --pretrained_path <path/to/pretrained/model/folder>
```
Use ```-h``` to see more choices for hyperparameters and other routes.
Then execute:
```shell
$ bash train.sh <path/to/config>
```

## Inference
You can choose whether to pass `$1` as the path to the output image folder. If it is not provided, the `./test_output` directory, located at the same level as the bash script, will automatically be used to store the output images.
```shell
$ bash inference.sh $1
```
Subfolders named `0`, `1`, `2`, and `3` will be created within the specified path to the output image folder, corresponding to the images generated by each prompt number.

For inference with a single configuration, do
```shell
$ python3 sample.py --config_file <path/to/config>
```
for models other than SDXL. For SDXL, do
```shell
$ python3 sample_sdxl.py --config_file <path/to/config>
```

### CodaLab final submission
```shell
$ bash inference_codalab.sh
```
Default outputs are under ```test_output_codalab``` folder.

### Peer review submission
```shell
$ bash inference_peer.sh
```
Default outputs are under ```test_output_peer``` folder.

## Poster results

### Ablation study

For CLIP score evaluation, we modify the HW2 grading scripts under ```clip_evaluation```. The results are not always the same but close to the score on CodaLab. To evaluate a folder containing images under subfolders ```0```,```1```,```2```,```3```, do:
```shell
$ python3 clip_evaluation/clip_image_score.py --input_dir Data/concept_image \
        --json_path Data/prompts.json --output_dir <path/to/folder>
```

Exp2: 2(+)3(-)4(-):
```shell
$ bash exp2.sh
```

Exp3: 2(+)3(+)4(-):
```shell
$ bash exp3.sh
```

Exp4: 2(+)3(+)4(+):
- Prompt 0, 1, 2: use results from Exp3. (no style needed)
- Prompt 3: Style injection on results from Exp3.
```shell
$ cd StyleID
$ bash run.sh
```

### Attention Clustering Post-processing example
```shell
$ bash scripts/attn_clustering.sh
```
The images are provided under ```source``` folder.

### Mix of Show
For reproduction of mix-of-show results, please clone this repository with ```--recursive``` to download the Mix-of-Show submodule.
Please follow the environment setup in Mix of Show repository.
```shell
$ cd Mix-of-Show
$ python3 setup.py install
    
# Clone diffusers==0.14.0 with T2I-Adapter support
$ git clone https://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p@github.com/guyuchao/diffusers-t2i-adapter.git

# switch to T2IAdapter-for-mixofshow
$ cd diffusers-t2i-adapter
$ git switch T2IAdapter-for-mixofshow

# install from source
$ pip install .
```
... etc.

To download checkpoints, execute:
```shell
$ cd experiments/pretrained_models
$ gdown 16P7v_WQ46csK_KfXhmkt1iO9ulpkUjq8
$ unzip composed_edlora.zip
$ rm composed_edlora.zip
```

Please also download the stable-diffusion v1.4 model, and create soft link:
```shell
$ ln -s <path/to/sd-v1-4> experiments/pretrained_models/stable-diffusion-v1-4
```
For inference, here we take the prompt-0 for example:
```shell
$ combined_model_root="experiments/composed_edlora/stable-diffusion-v1-4/"
$ expdir="cat2+dog6"

$ context_prompt="A <cat2> on the right and a <dog6> on the left."
$ python3 Mix-of-Show/inference/mix_of_show_sample.py \
        --pretrained_model="experiments/pretrained_models/stable-diffusion-v1-4" \
        --combined_model="${combined_model_root}/${expdir}/combined_model_.pth" \
        --save_dir="results/multi-concept/${expdir}" \
        --pipeline_type="sd_pplus" \
        --prompt="${context_prompt}" \
        --suffix="" \
        --n_samples=<n_sample_you_want>
```
### Style Mix
You can choose whether to pass `$1` as the path to the output image folder. If it is not provided, the `./style_mix_output` directory, located at the same level as the bash script, will automatically be used to store the output images.
```bash
$ bash inference_style_vango.sh
```
`./style_mix_output/independent` directory will be created to save the output of the setting that `<watercolor>` and `<Vango>` weight is trained independently.

`./style_mix_output/mix` directory will be created to save the output of the setting that `<watercolor>` and `<Vango>` weight is shared.