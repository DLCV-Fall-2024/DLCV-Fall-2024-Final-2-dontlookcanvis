# DLCV Final Project ( Multiple Concept Personalization )

## Checkpoints download
Please download all the checkpoints from: https://drive.google.com/drive/folders/1u2un5sqWEY7yj-U-1wgyN41BICqQ3wWm?usp=sharing
You can also use ED-LoRA directly from this repository; only merged LoRAs need downloads.

## Clone this repo

    git clone https://github.com/DLCV-Fall-2024/DLCV-Fall-2024-Final-2-dontlookcanvis.git


## Preparation
**Download Pretrained Model**
```shell
$ cd experiments/pretrained_models
```
Execute in `./experiments/pretrained_models`
```shell
$ git-lfs clone https://huggingface.co/windwhinny/chilloutmix.git
$ git clone https://huggingface.co/sd-legacy/stable-diffusion-v1-5
$ git-lfs clone https://huggingface.co/SG161222/RealVisXL_V5.0.git
```

**Install environment**
In the repository folder,
```shell
$ pip install -r requirements.txt --extra-index-url https://download.pytorch.org/whl/cu118
```

## Training
Directly use the provided training configs, or generated needed files and configuration from template as:
``` shell
$ python scripts/data_format.py --input_images <path/to/images/folder> \
                                --input_json_path <path/to/annotation/file> \
                                --pretrained_path <path/to/pretrained/model/folder>
```
Use ```-h``` to see more choices for hyperparameters and other routes.
Then execute:
```shell
$ bash train.sh <path/to/config>
```

## Inference
You can choose whether to pass `$1` as the path to the output image folder. If it is not provided, the `./test_output directory`, located at the same level as the bash script, will automatically be used to store the output images.
```shell
$ bash inference.sh $1
```
Subfolders named `0`, `1`, `2`, and `3` will be created within the specified path to the output image folder, corresponding to the images generated by each prompt number.

## Reproduce experimental results
### Peer review

    cd Concept-Conductor
    bash ../scripts/inf0_2.sh
    bash ../scripts/inf2_2.sh


### Final CodaLab uploads
0.

1.

2.

3.


### Poster results

#### Attention Clustering Post-processing
```shell
$ bash scripts/attn_clustering.sh
```

#### Mix of Show
For reproduction of mix-of-show results, please clone this repository with ```--recursive``` to download the Mix-of-Show submodule.
Please follow the environment setup in Mix of Show repository.
```shell
$ cd Mix-of-Show
$ python setup.py install
    
# Clone diffusers==0.14.0 with T2I-Adapter support
$ git clone https://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p@github.com/guyuchao/diffusers-t2i-adapter.git

# switch to T2IAdapter-for-mixofshow
$ cd diffusers-t2i-adapter
$ git switch T2IAdapter-for-mixofshow

# install from source
$ pip install .
```
... etc.

To download checkpoints, execute:
```shell
$ cd experiments/pretrained_models
$ gdown 16P7v_WQ46csK_KfXhmkt1iO9ulpkUjq8
$ unzip composed_edlora.zip
$ rm composed_edlora.zip
```

Please also download the stable-diffusion v1.4 model, and create soft link:
```shell
$ ln -s <path/to/sd-v1-4> experiments/pretrained_models/stable-diffusion-v1-4
```
For inference, here we take the prompt-0 for example:
```shell
$ combined_model_root="experiments/composed_edlora/stable-diffusion-v1-4/"
$ expdir="cat2+dog6"

$ context_prompt="A <cat2> on the right and a <dog6> on the left."
$ python Mix-of-Show/inference/mix_of_show_sample.py \
        --pretrained_model="experiments/pretrained_models/stable-diffusion-v1-4" \
        --combined_model="${combined_model_root}/${expdir}/combined_model_.pth" \
        --save_dir="results/multi-concept/${expdir}" \
        --pipeline_type="sd_pplus" \
        --prompt="${context_prompt}" \
        --suffix="" \
        --n_samples=<n_sample_you_want>
```
